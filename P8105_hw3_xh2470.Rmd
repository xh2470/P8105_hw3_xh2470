---
title: "P8105_hw3_xh2470"
author: "Xueqing Huang"
output: github_document
---

# Problem 1

```{r}
options(warn = -1)
library(tidyverse)
library(p8105.datasets)
data("instacart")
```

Write a short description of the dataset.

This dataset contains more than 3 million online grocery orders from more than 200,000 Instacart users, with  **`r nrow(instacart)`** rows and **`r ncol(instacart)`** columns. Variables contain _`r names(instacart)`_.  Some key observations are:
`r knitr::kable(head(instacart))` 

Take the first observation as an example. Bulgarian Yogurt which belonged to the department of dairy eggs was ordered. Also, it gave us the information of the day of the week and the hour of the day on which the order was placed.

Then, do or answer the following (commenting on the results of each):

1. How many aisles are there, and which aisles are the most items ordered from?

```{r}
aisle =
  instacart  %>%  
  group_by(aisle) %>% 
  summarize(n_obs = n()) 
  
aisle_rank = 
  aisle %>% 
  mutate(rank = min_rank(desc(n_obs))) %>% 
  filter(rank == 1)
```

There are **`r nrow(aisle)`** aisles and the most items ordered from is **`r pull(aisle_rank, aisle)`**.

2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>%
  filter(n_obs > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n_obs)) %>% 
  ggplot(aes(x = n_obs, y = aisle)) +
  geom_point(alpha = .5) +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Numer of items",
    y = "Aisle",
    caption = "Only contain aisles with more than 10000 items."
  ) 
```

Among aisles contain more than 10000 items, fresh vegetables contains the most items and butter contains the least items.

3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r}
instacart  %>%  
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(number = n()) %>% 
  mutate(ranking = min_rank(desc(number))) %>% 
  filter(ranking < 4) %>% 
  arrange(aisle, ranking) %>% 
  knitr::kable()
```

The most popular item in the category of baking ingredients, dog food care, and packaged vegetables fruits is Light Brown Sugar, Snack Sticks Chicken & Rice Recipe Dog Treats, and Organic Baby Spinach, respectively.

4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart  %>%  
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>%
  summarize(
    mean = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean
  ) %>% 
  knitr::kable()
```

Customers who ordered Pink Lady Apples and Coffee Ice Cream prefer to place their orders in the middle of the day. 


# Problem 2

First, do some data cleaning:

```{r}
data("brfss_smart2010") 

brfss_smart2010 = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health", response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good","Excellent" )))   #organize responses as a factor 

brfss_smart2010

```

Using this dataset, do or answer the following (commenting on the results of each):

1. In 2002, which states were observed at 7 or more locations? What about in 2010?

```{r}
p2_q1 = #problem2_question1
  brfss_smart2010 %>% 
  group_by(year, locationabbr) %>% 
  summarize(n_obs = n_distinct(locationdesc))

# year = 2002
p2_q1 %>% 
  filter(year == 2002, n_obs > 6) %>% 
  knitr::kable()

# year = 2010
p2_q1 %>% 
  filter(year == 2010, n_obs > 6) %>% 
  knitr::kable()
```

In 2002, there were **6** states that were observed at 7 or more locations, which were **CT, FL, MA, NC, NJ, and PA**. In 2010, there were **14** states that were observed at 7 or more locations, which were **CA, CO,FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA**. 

2. Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
p2_q2 = # problem2_qustion2
  brfss_smart2010 %>% 
  filter(response == "Excellent") %>%
  group_by(year, locationabbr) %>% 
  summarize(
    mean = mean(data_value)
  ) 

p2_q2

p2_q2 %>% 
  group_by(year, locationabbr) %>% 
  ggplot(aes(x = year, y = mean, color = locationabbr)) +
  geom_point(alpha = .5) +
  geom_line() +
  labs(
    title = "Average Value Over Time",
    x = "Year",
    y = "Average Value"
  ) 
  
```

The distribution of the average value over time within a state is concentrated.

3. Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
brfss_smart2010 %>% 
  filter(year %in% c(2006, 2010), locationabbr == "NY") %>%
  ggplot(aes(x = data_value, y = locationdesc, color = response)) + 
  geom_point() +
  facet_grid(. ~ year) +
  labs(
    title = "Distribution of data_value for Responses in 2006 and 2010"
  ) 
```

Whether in the year of 2006 or 2010, data_value for responses among locations in NY State have little difference.

# Problem 3

1.Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

```{r}
data = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(weekday_vs_weekend = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")) %>%      # add weekday_vs_weekend variable
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_time",
    values_to = "activity_value"
  ) %>% 
  separate(activity_time, into = c("activity", "minute"), sep = 9) %>%  # simplify activity_time
  select(-activity) 

data
```

This dataset collects accelerometer data of a 63 year-old male with BMI 25. It shows the activity value for each minute of a 24-hour day for the duration of 5 weeks. It contains   **`r nrow(data)`** rows and **`r ncol(data)`** columns. Variables contain _`r names(data)`_.


2.Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r}
p3_q2 =  # Problem3_question2
  data %>% 
  group_by(day_id) %>%
  summarize(total = sum(activity_value)) 

knitr::kable(p3_q2) 

ggplot(p3_q2, aes(x = day_id, y = total)) +
  geom_point(alpha = .5) +
  geom_line() +
  labs(
    title = "Total Activity Value of Each Day",
    x = "Day",
    y = "Total Activity Value "
  ) +
  scale_x_continuous(
    breaks = c(0,5,10,15,20,25,30,35))
```

The activity value of this person was fluctuated. In general, he was more active in the first half of the study than the second half.  

3. Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r}
data %>% 
  mutate(hour = floor(as.numeric(minute)/60),
         day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  group_by(day_id, day, hour) %>% 
  summarize(activity_of_hour = sum(activity_value)) %>% 
  ggplot(aes(x = hour , y = activity_of_hour, color = day)) +
  geom_line() + 
  labs(
    title = "Activity Value of a Day",
    x = "Hour",
    y = "Activity Value "
  ) +
  scale_x_continuous(
    breaks = c(0,4,8,12,16,20,24)
    )
```

This plot shows the 24-hour activity time courses of this person. From the plot, we can conclude that this person was more acted at night than other time on weekday, and more acted in the morning and noon than other time on weekend.  